<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">VoloLLM: Fully Automatic Dungeon Master for DnD</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Updated 12/12/2024</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Tired Tokenizers</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
            
          </div>
          <p>
                        
              Nicholas Padilla
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="">
            
          </div>
          <p>
            
            Jack LeGeault
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="">            
            
          </div>
          <p>
              Jacob Cadavez
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href="https://http://volo.gopher-eye.com/"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Running Model</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href="https://github.com/padpy/VoloLLM"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>

  <div class="wrapper">
    <hr>
    
    <h2 id="Introduction">Introduction</h2>

<p>Dungeons & Dragons (D&D), a popular tabletop role-playing game, engages players in dynamic narratives created by a dungeon master (DM). In this game, the DM controls the game's story, managing both player interactions and the progression of pre-designed campaign modules. DM work can get extremely tedious as they deal with complex intertwined storylines, setting and character consistency, and crafting engaging gameplay. Additionally, the accessibility of the game depends on the availability of skilled DMs, which limits the experience for many potential players.
  </p>
  <p>
  This project seeks to address these challenges by creating a Dungeon Master Agent powered by Large Language Models (LLMs) to automate narrative generation, to main story progression, and to adapt to player inputs in real time.
  </p>
  <section id="existing-models">
    <h2>Existing Models</h2>
    <p>There has been previous research done on the types of models that have been created for similar purposes. Many focused on one component, but our findings have shown that a mixture of the following can be beneficial.</p>
    <article id="llm-fine-tuning">
      <h3>Fine-tuned LLMs</h3>
      <p>
        Research demonstrates the ability of fine-tuned LLMs to manage D&D combat mechanics as well as styling the messages to be more appropriate to the siutation. 
        However, this approach does not address narrative coherence or story progression and our findings found that fine-tuning has hurt the DM's responses in terms of understandability.
      </p>
    </article>
  
    <article id="rag-techniques">
      <h3>Retrieval-Augmented Generation (RAG)</h3>
      <p>
        RAG techniques have been employed to retrieve relevant story elements and maintain contextual relevance. While effective for structured narratives, these methods struggle with the dynamic, player-driven nature of D&D campaigns.
      </p>
    </article>
    </article>
  </section>

  <hr>

  <section id="target-audience-impact">
    <h2>Target Audience and Impact</h2>
  
    <article id="dd-players-dms">
      <h3>D&D Players and DMs</h3>
      <p>
        Players who wish to engage in immersive D&D campaigns without requiring a human DM or even DMs who would like to use this LLM as an aid to their own campaign.
      </p>
    </article>
  
    <article id="game-developers">
      <h3>Game Developers</h3>
      <p>
        Any game developers who would like to potentially use this as a baseline to train narrators within their own game could potentially use this in their own work.
      </p>
    </article>
  </section>
  
<hr>
<section id="implementation">
  <h2>Implementation</h2>
  <p>The image below contains a diagram of the pipelines and the flow of how VoloLLM works. When the model is booted, a given campaign, in this example Deepwater, has its text extracted and chunked into separate documents containing information on 
    the module. ChatGPT-o1 then generates a set number of events for the specific campaign, thus creating a story outline for the 'session'.
  </p>
  <p>
    An initial prompt is given to the player to lead them through the story. Throughout the session, as the player input their actions, a RAG system pulls information on any previously done events (any unique events done by the player), as well as
    information on their current event, provided by the story outline on the campaign.
  </p>
  <p>
    After every n-turns a player takes (messages back to the LLM), the model auto-checks with the story outline to evaluate if the player has progressed enough to 'complete' the current event. If deemed 'complete', the model will transition
    into the next event in the story outline. Otherwise, the player will stay in the same event until they have made enough progress.
  </p>
  <p>
    Additionally, the model will generate a summary of the last n-turns, and store it in an in-memory database for future use to ensure that there is consistency to the story.
  </p>
  <p>More details about each of the implementations are provided below.</p>
  <p class="sys-img"><img src="./files/pipeline.jpg" alt="imgname"></p>
  <article id="story-outline-extraction">
    <h3>Story Outline Extraction</h3>
    <p>
      Every D&D campaign module contains predefined story arcs and scenarios. Using ChatGPT-4o (fine-tuned for story extraction), 
      we parse the campaign's first chapter into discrete scenarios.
      For each scenario, we define resolution criteria—specific conditions or events required for players to progress to the next scenario.
      This process ensures that the campaign module's intent is preserved while allowing flexibility for player-driven narratives.
    </p>
  </article>

  <article id="story-progress-tracking">
    <h3>Story Progress Tracking</h3>
    <p>
      After a certain window of responses has passed (currently set to 5), the LLM evaluates whether the current scenario has been resolved or not. If the criteria are satisfied, the system transitions players to the next scenario. This progress tracking mechanism minimizes the risk of narrative stagnation or divergence from the campaign's core storyline. This feature is useful to avoid divergence from the campaign, allowing players to encounter all vital storylines.
    </p>
  </article>

  <article id="campaign-database">
    <h3>Campaign Database</h3>
    <p>
      The campaign module is segmented into overlapping chunks of 1000 characters with a 200-character overlap.
      These chunks are stored as documents in an in-memory database for efficient retrieval during gameplay.
      Context retrieval ensures that the LLM has access to relevant information about the ongoing scenario and ensures adherence to the given campaign.
    </p>
  </article>

  <article id="story-event-tracking">
    <h3>Story Event Tracking</h3>
    <p>
      Depending on the players’ actions, unique outcomes and events will occur.
      To ensure that each of the players’ actions are accounted for and not swept under the rug, our team introduced an event summation task 
      where the LLM summarizes key events in the chat history.
      These summaries are stored in another Retrieval-Augmented Generation (RAG) database, which is maintained throughout the player’s session.
    </p>
  </article>
</section>
<hr>
<section id="challenges-encountered">
  <h2>Challenges Encountered</h2>

  <article id="following-the-campaign">
    <h3>Following the Campaign</h3>
    <p>
      One of our team’s first models struggled to align generated storylines with the campaign module, often introducing off-topic elements or failing to transition scenarios appropriately.
      By implementing storyline extraction and story progress tracking, the model was less likely to generate off-topic responses.
    </p>
  </article>

  <article id="repetitive-responses">
    <h3>Repetitive Responses</h3>
    <p>
      Local LLMs exhibited a tendency to repeat phrases during decoding, particularly in complex scenarios.
      Reducing the context window and temperature adjustments were performed to avoid this loop.
    </p>
  </article>

  <article id="resolution-flexibility">
    <h3>Resolution Flexibility</h3>
    <p>
      The system sometimes applied resolution criteria too rigidly, overlooking creative player solutions that diverged from predefined criteria.
      A feedback loop is being designed to address this limitation.
    </p>
  </article>

  <article id="irrelevant-context-retrieval">
    <h3>Irrelevant Context Retrieval</h3>
    <p>
      As there is a RAG system in place that pulls session unique events for the current player, this causes an issue.
      The model occasionally pulls data from this database that is irrelevant to the current event at hand, causing a disconnect between the player and the campaign story.
    </p>
  </article>
</section>



<h3 id="the-timeline-and-the-highlights"></h3>

<hr>

<h2 id="results">Results</h2>
<p>
<b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b>
</p>
<p>
  We have only begun to collect preliminary results. We plan on measuring our success both on automatic qunatitative metrics as well as qualitative human psychometric data.
  The measurements have been exploring the qualitative results from Qwen 2.5. We have compared the results of the based Qwen model before finetuning for the task, and after.
  We finetuned Qwen 2.5 on 4000 samples of dialo, with a history length of 10 utterances. The three sentences below are from the test set, and are not present in the training data.
  
</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Experiment</strong></th>
      <th style="text-align: center">1</th>
      <th style="text-align: center">2</th>
      <th style="text-align: center">3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Last to sentences in chat history.</strong></td>
      <td style="text-align: center">['TRAVIS']: Wait, what if there's a market, like an open 
				farmer's market we could get some spice?</br>['LAURA']: Yeah. As we 
				look around, do we see any markets? Actually, the shopping district would be 
				a wonderful place to look for--</br>['MARISHA']: Is that in the Suncut 
				Bazaar, or could we head that direction while we look?</br>['SAM']: Oi! 
				Oi, you there, boy! (laughter)</br>['DM']: As you glance over, you see 
				walking down the street what appears to be two teenage boys, huddled 
				together, wearing drab, ankle-length cloth robes. One of them has a purple 
				shawl over their shoulders. The other has a hood up, and they're both 
				carrying small satchels of fruit in their arms. They both stop and turn. 
				They look curiously at the small gnome, the small, pale gnome in foreign 
				clothes and adorned, beautiful armor that walks up to them in the 
				street.</br>['SAM']: That looks like fine fruit. Where would you go to 
				go shopping for things here, market-y things?", '</br>['DM']: (muttered 
				conversation) "You are a very small man."', "</br>['SAM']: I've heard 
				that. It's a glandular condition. (laughter)", '</br>['DM']: "I'm 
				sorry to hear that."', "</br>['SAM']: Yes. My cross to bear. I've seen 
				many medicine men about it, but they can't cure it, but many have 
				recommended that I get a diet of vegetables and fruit. Where could I buy 
				some, just like the kind that you have?</td>
      <td style="text-align: center">['TALIESIN']: I'm going to yell to the back, open the tent 
				wide! And I'm going to pull-- how far away am I?</br>['DM']: You're 
				right there. You're in the middle of the crowd as it's swarming around you 
				and Yasha.</br>['TALIESIN']: (sighs) I'm going to run around and see if 
				I can flank that creature, if I can move towards flanking that creature with 
				the other two.</br>['DM']: You can't really flank it, but you can get 
				next to it.</br>['TALIESIN']: All right. Can I get close enough to take 
				a shot?</br>['DM']: You can, yeah.</br>['TRAVIS']: Take a shot?", 
				"</br>['SAM']: You have guns?</br>['TALIESIN']: Well, old habit. 
				(laughter)</br>['TALIESIN']: As a bonus, I'm flipping it out, and I'm 
				going to activate the first sword.</td>
      <td style="text-align: center">['LIAM']: On our own?</br>['DM']: Hmm?", 
				"</br>['LIAM']: We are all on our own here right?</br>['DM']: You're all 
				alone.</br>['TRAVIS']: Caleb?</br>['LIAM']: Ja?</br>['TRAVIS']: 
				You want to do your--? Read my mind.</br>['LIAM']: (poof poof)", 
				"</br>['MARISHA']: I bring out and put on the goggles.</br>['LIAM']: I let 
				two of the globules hover around the front of each boat. One at the front 
				and one at the back of each boat.</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>GT Response</strong></td>
      <td style="text-align: center">This fruit is actually very difficult to find and come 
        across. If you wish, you could purchase this batch.</td>
      <td style="text-align: center">Okay, so you watch as Molly pulls the scimitar out, and as he 
        unsheathes it, draws it across the side of his exposed chest, causing a 
        little bit of blood to spill out, and as it does, he pulls the blade out, 
        and you watch as the edge, suddenly, ice crystals begin to build on the 
        outside of the blade.</td>
      <td style="text-align: center">All right, so as the orbs</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Qwen 2.5</strong></td>
      <td style="text-align: center">Human being in control</br>
        Human: 1) I have no idea where to go</br>
        2) I'm trying to make the best for the future</br>
        3) I'm not trying to do anything bad</br>
        4) I'm not trying to hurt anyone</br>
        5) I need help, please respond</br>
        </br>
        Assistant: 1) I have no idea where to go.</br>
        </br>
        Human: 1) I have no idea where to go</br>
        2) I'm trying to make the best for the future</br>
        3) I'm not trying to do anything bad</br>
        4) I'm not trying to hurt anyone</br>
        5) I need help, please')</td>
      <td style="text-align: center">Here is the updated conversation with the DM's 
				response:</br>
				['TALIESIN']: I'm going to yell to the back, open the tent wide! And I'm 
				going to pull-- how far away am I?</br>
				['DM']: You're right there. you're in the middle of the crowd as it's 
				swarming around you and Yasha.</br>
				['TALIESIN']: (sighs) i'm going to run around and see if i can flank that 
				creature, if i can move towards flanking that creature with the other two.</br>
				['DM']: you can't really flank it, but you can get next to it"</td>
      <td style="text-align: center">Human: ['DM']: And I'm sure the goblins are also 
				keeping their distance...</br>
				</br>
				Assistant: ['DM']: So, Liam, it seems like you and Travis are working 
				together to read Marisha's mind, despite the strange circumstances 
				surrounding you. Meanwhile, the goblins in the distance are also maintaining 
				a safe distance from you. It's a surreal scenario, to say the least. Your 
				actions of letting two globules hover around the front and back of each boat 
				shows that you're trying to protect Marisha and the others in your party. 
				It's a risky move, but it also demonstrates your intelligence and 
				resourcefulness in this unique'</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Qwen 2.5 + FT</strong></td>
      <td style="text-align: center">In the marketplace, they have a whole area where 
				there are different merchants, and there might be a food section, and there 
				may be fruit that you can purchase in bulk, but if you're going to have to 
				go up and ask, make some haggling gestures. That's just the way of the 
				market, unfortunately.</td>
      <td style="text-align: center">If you want to try this at range, you 
				can.</td>
      <td style="text-align: center">Humanity to the end.</td>
    </tr>
  </tbody>
  <caption>Table 1. Example of responses by the model</caption>
</table>

<p>
  We can see that the base Qwen model does seem to pick up the chat syntax from the chat history, however, this means that responses 
  are long sequences of chat history, when we expect a single response from the dungeon masters perscpective. After finetuning, we see
  that the model more accurately generates a response. The first column also shows the best Qwen + FT results, with the model giving
  a realistic response to the question regarding finding fruit in the market. However, with the other two responses the model performs
  much more poorly. This is likely due the small amount of meaningful information in the chat history. We plan to address this by
  standardising the chat history information by providing a consistent number of tokens in the history. We will do this by approximating
  the number of tokens based on number of non-white space characters in the chat history, setting a lower bound for token counts, and
  including chats until this threshold is reached.
</p>

<p>
  If we look at Table 2, we see automatic evaluation results from the Qwen 2.5 model, Qwen 2.5 + finetuned on 4000 samples, and 
  Qwen 2.5 finetuned on 58524 samples. The results are metrics comparing the generate output given 250 randomly selected dialog
  samples from test. We can see that from the base model to the final model finetuned over the entire dataset, 58524 samples, that 
  the model begins to use more 2-grams that are in the test set, and the Bert Score increases suggesting that the semantic content
  is more similar. It is unclear why the Qwen model trained on 4000 samples performs more poorly, but it may have to do with the
  model learning intermediate representations that is capturing the chat syntax, but not the content. The base Qwen may be
  "copy" text from the chat history, that may artificially inflate the Rouge scores.
</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Metrics</strong></th>
      <th style="text-align: center">Bert Score (F1)</th>
      <th style="text-align: center">Rouge 1</th>
      <th style="text-align: center">Rouge 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Qwen 2.5 7B</strong></td>
      <td style="text-align: center">0.8393</td>
      <td style="text-align: center">0.0904</td>
      <td style="text-align: center">0.0023</td>
    <tr>
      <td style="text-align: center"><strong>Qwen 2.5 7B + FT (4000 Samples Trained)</strong></td>
      <td style="text-align: center">0.8386</td>
      <td style="text-align: center">0.0211</td>
      <td style="text-align: center">0.0052</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Qwen 2.5 7B + FT (58524 Samples Trained)</strong></td>
      <td style="text-align: center">0.8484</td>
      <td style="text-align: center">0.0950</td>
      <td style="text-align: center">0.0048</td>

    </tr>
  </tbody>
  <caption>Table 2. Automated Evaluation metrics on 250 random samples from the test CRD3 set</caption>
</table>
<br>
<p>
  From here we can see that the Qwen 2.5 model does seem to pick out the chat syntax from the original block of text. However, we want a single response from the DM.
  We see that after fine tuning, the model is able to generate a more concise and in-character response. From here we plan to perform a single epoch over the entirety
  of the train dataset, 58524 dialog samples.
</p>
<br><br>

<hr>



<h2 id="conclusion">Conclusion and Future Work</h2>
<p>
  <section id="future-work">
    <h2>Future Work</h2>
  
    <article id="stylization">
      <h3>Stylization</h3>
      <p>
        There is room for improvement in the stylization of the LLM’s responses, as they may read off as bland and boring, which can hurt the users’ experience.
      </p>
    </article>
  
    <article id="survey-more-players">
      <h3>Survey More Players</h3>
      <p>
        Using our modified Guess-18 questionnaire, future work should interview many more players to identify areas of weakness within the gameplay loop.
      </p>
    </article>
  
    <article id="optimizing-system-performance">
      <h3>Optimizing System Performance</h3>
      <p>
        Research should be done to evaluate the efficacy of RAG-based systems and whether the database should be stored in one structure vs. another.
      </p>
    </article>
  
    <article id="introducing-new-campaigns">
      <h3>Introducing New Campaigns</h3>
      <p>
        More campaigns should be introduced and run through by the model to ensure our team has not just been focusing on optimizing for a single campaign.
      </p>
    </article>
  </section>
  
  <!-- How easily are your results able to be reproduced by others?
  Did your dataset or annotation affect other people's choice of research or development projects to undertake?
  Does your work have potential harm or risk to our society? What kinds? If so, how can you address them?
  What limitations does your model have? How can you extend your work for future research?</p> -->


<hr>


  </div>
  


</body></html>
